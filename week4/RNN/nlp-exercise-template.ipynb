{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c19d029-25f5-4d8e-9514-bf3ad9d30026",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/Cyrille/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/Cyrille/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/Cyrille/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "2023-12-19 21:58:50.627773: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e2401a-3b28-4a42-ad99-4623e795c5b3",
   "metadata": {},
   "source": [
    "# Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5766eb4-ca56-44be-9c47-23a102b3d3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('../../data/imdb_train_data_small.csv')\n",
    "test_df = pd.read_csv('../../data/imdb_test_data_small.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6342a5a9-2277-422f-96af-351bad324de7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This movie has bad writing and bad editing. It...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I'm still laughing- Not! I'm still asking my m...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>While I'm normally a big fan of John Turturro'...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;br /&gt;&lt;br /&gt;The author tried to make a Kevin S...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Oh boy ! It was just a dream ! What a great id...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>My wife and I struggle to find movies like thi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>While watching this film recently, I constantl...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>Trust the excellent and accurate Junagadh75 re...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>Valley Girl is an exceptionally well made film...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>\"Just before dawn \" is one of the best slasher...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  label\n",
       "0    This movie has bad writing and bad editing. It...      0\n",
       "1    I'm still laughing- Not! I'm still asking my m...      0\n",
       "2    While I'm normally a big fan of John Turturro'...      0\n",
       "3    <br /><br />The author tried to make a Kevin S...      0\n",
       "4    Oh boy ! It was just a dream ! What a great id...      0\n",
       "..                                                 ...    ...\n",
       "295  My wife and I struggle to find movies like thi...      1\n",
       "296  While watching this film recently, I constantl...      1\n",
       "297  Trust the excellent and accurate Junagadh75 re...      1\n",
       "298  Valley Girl is an exceptionally well made film...      1\n",
       "299  \"Just before dawn \" is one of the best slasher...      1\n",
       "\n",
       "[300 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "26271f3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I rated this a 3. The dubbing was as bad as I ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;br /&gt;&lt;br /&gt;Cheap-looking and ugly, this film ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This film concerns purportedly non-establishme...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ho-hum. An inventor's(Horst Buchholz)deadly bi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Definitely not worth the rental, but if you ca...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>695</th>\n",
       "      <td>This has to be the funniest stand up comedy I ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>696</th>\n",
       "      <td>. . . is just as good as the original. Very ne...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>697</th>\n",
       "      <td>A quite good film version of the novel, though...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>698</th>\n",
       "      <td>Maybe the greatest film ever about jazz.&lt;br /&gt;...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699</th>\n",
       "      <td>Kept my attention from start to finish. Great ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>700 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  label\n",
       "0    I rated this a 3. The dubbing was as bad as I ...      0\n",
       "1    <br /><br />Cheap-looking and ugly, this film ...      0\n",
       "2    This film concerns purportedly non-establishme...      0\n",
       "3    Ho-hum. An inventor's(Horst Buchholz)deadly bi...      0\n",
       "4    Definitely not worth the rental, but if you ca...      0\n",
       "..                                                 ...    ...\n",
       "695  This has to be the funniest stand up comedy I ...      1\n",
       "696  . . . is just as good as the original. Very ne...      1\n",
       "697  A quite good film version of the novel, though...      1\n",
       "698  Maybe the greatest film ever about jazz.<br />...      1\n",
       "699  Kept my attention from start to finish. Great ...      1\n",
       "\n",
       "[700 rows x 2 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a2429687",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[\"label\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693a8840-336c-4b42-b6b8-2c4cc53f229f",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "\n",
    "Create your own tokenization algorithm. Remember to handle upper/lower case, comma, punctioation and so on.\n",
    "Each word should hava an integer connected to it. Word as key and integer as value in a dict is one way to do it.\n",
    "\n",
    "Tensorflow have tokenization models, but try to bild it yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "caa2cc1d-9051-4db9-8fc5-a939df97a5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def tokenize(dataset: pd.DataFrame):\n",
    "\n",
    "    text = dataset[\"text\"]\n",
    "    text = text.str.lower()\n",
    "    text = text.str.replace(r'[^a-z0-9\\s]', '', regex=True)\n",
    "    \n",
    "    words_series = text.str.split()\n",
    "    \n",
    "    token_map = {\"<UNK>\": 0}\n",
    "    reverse_token_map = {0: \"<UNK>\"}\n",
    "    next_token = 1\n",
    "    \n",
    "    for word_list in words_series:\n",
    "        for word in word_list:\n",
    "            if word not in token_map:\n",
    "                token_map[word] = next_token\n",
    "                reverse_token_map[next_token] = word\n",
    "                next_token += 1\n",
    "    \n",
    "    return token_map, reverse_token_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7be5fc56",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_map, reverse_token_map = tokenize(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2db8239",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<UNK>': 0,\n",
       " 'i': 1,\n",
       " 'rated': 2,\n",
       " 'this': 3,\n",
       " 'a': 4,\n",
       " '3': 5,\n",
       " 'the': 6,\n",
       " 'dubbing': 7,\n",
       " 'was': 8,\n",
       " 'as': 9,\n",
       " 'bad': 10,\n",
       " 'have': 11,\n",
       " 'seen': 12,\n",
       " 'plot': 13,\n",
       " 'yuck': 14,\n",
       " 'im': 15,\n",
       " 'not': 16,\n",
       " 'sure': 17,\n",
       " 'which': 18,\n",
       " 'ruined': 19,\n",
       " 'movie': 20,\n",
       " 'more': 21,\n",
       " 'jet': 22,\n",
       " 'li': 23,\n",
       " 'is': 24,\n",
       " 'definitely': 25,\n",
       " 'great': 26,\n",
       " 'martial': 27,\n",
       " 'artist': 28,\n",
       " 'but': 29,\n",
       " 'ill': 30,\n",
       " 'stick': 31,\n",
       " 'to': 32,\n",
       " 'jackie': 33,\n",
       " 'chan': 34,\n",
       " 'movies': 35,\n",
       " 'until': 36,\n",
       " 'somebody': 37,\n",
       " 'tells': 38,\n",
       " 'me': 39,\n",
       " 'jets': 40,\n",
       " 'english': 41,\n",
       " 'up': 42,\n",
       " 'par': 43,\n",
       " 'br': 44,\n",
       " 'cheaplooking': 45,\n",
       " 'and': 46,\n",
       " 'ugly': 47,\n",
       " 'film': 48,\n",
       " 'didnt': 49,\n",
       " 'even': 50,\n",
       " 'seem': 51,\n",
       " 'entertain': 52,\n",
       " 'kids': 53,\n",
       " 'in': 54,\n",
       " 'audience': 55,\n",
       " 'except': 56,\n",
       " 'for': 57,\n",
       " 'one': 58,\n",
       " 'fairly': 59,\n",
       " 'amusing': 60,\n",
       " 'toilet': 61,\n",
       " 'joke': 62,\n",
       " 'christopher': 63,\n",
       " 'lloyd': 64,\n",
       " 'way': 65,\n",
       " 'past': 66,\n",
       " 'his': 67,\n",
       " 'prime': 68,\n",
       " 'actually': 69,\n",
       " 'quite': 70,\n",
       " 'tiresome': 71,\n",
       " 'role': 72,\n",
       " 'although': 73,\n",
       " 'sorry': 74,\n",
       " 'excuse': 75,\n",
       " 'jokes': 76,\n",
       " 'by': 77,\n",
       " 'writers': 78,\n",
       " 'dont': 79,\n",
       " 'help': 80,\n",
       " 'elizabeth': 81,\n",
       " 'hurley': 82,\n",
       " 'embarrassingly': 83,\n",
       " 'amateurish': 84,\n",
       " 'supposedly': 85,\n",
       " 'comic': 86,\n",
       " 'jeff': 87,\n",
       " 'daniels': 88,\n",
       " 'darryl': 89,\n",
       " 'hannah': 90,\n",
       " 'avoid': 91,\n",
       " 'humiliation': 92,\n",
       " 'there': 93,\n",
       " 'really': 94,\n",
       " 'no': 95,\n",
       " 'reason': 96,\n",
       " 'make': 97,\n",
       " 'especially': 98,\n",
       " 'since': 99,\n",
       " 'it': 100,\n",
       " 'unavoidable': 101,\n",
       " 'that': 102,\n",
       " 'will': 103,\n",
       " 'compare': 104,\n",
       " 'with': 105,\n",
       " 'robin': 106,\n",
       " 'williamss': 107,\n",
       " 'often': 108,\n",
       " 'brilliant': 109,\n",
       " 'improvisations': 110,\n",
       " 'mork': 111,\n",
       " 'mindybr': 112,\n",
       " 'concerns': 113,\n",
       " 'purportedly': 114,\n",
       " 'nonestablishment': 115,\n",
       " 'types': 116,\n",
       " 'aesthetically': 117,\n",
       " 'sexually': 118,\n",
       " 'who': 119,\n",
       " 'apparently': 120,\n",
       " 'cannot': 121,\n",
       " 'resist': 122,\n",
       " 'basic': 123,\n",
       " 'romantic': 124,\n",
       " 'needs': 125,\n",
       " 'some': 126,\n",
       " 'excellent': 127,\n",
       " 'players': 128,\n",
       " 'take': 129,\n",
       " 'part': 130,\n",
       " 'including': 131,\n",
       " 'jon': 132,\n",
       " 'tenney': 133,\n",
       " 'timothy': 134,\n",
       " 'olyphant': 135,\n",
       " 'cynthia': 136,\n",
       " 'nixon': 137,\n",
       " 'they': 138,\n",
       " 'are': 139,\n",
       " 'grounded': 140,\n",
       " 'puerile': 141,\n",
       " 'script': 142,\n",
       " 'relies': 143,\n",
       " 'nearly': 144,\n",
       " 'totally': 145,\n",
       " 'upon': 146,\n",
       " 'clever': 147,\n",
       " 'dialogue': 148,\n",
       " 'isnt': 149,\n",
       " 'nixons': 150,\n",
       " 'possesses': 151,\n",
       " 'best': 152,\n",
       " 'lines': 153,\n",
       " 'she': 154,\n",
       " 'homes': 155,\n",
       " 'on': 156,\n",
       " 'them': 157,\n",
       " 'too': 158,\n",
       " 'quickly': 159,\n",
       " 'timing': 160,\n",
       " 'flaw': 161,\n",
       " 'must': 162,\n",
       " 'be': 163,\n",
       " 'saddled': 164,\n",
       " 'director': 165,\n",
       " 'grotesque': 166,\n",
       " 'climax': 167,\n",
       " 'utilizes': 168,\n",
       " 'every': 169,\n",
       " 'available': 170,\n",
       " 'cliche': 171,\n",
       " 'spent': 172,\n",
       " 'or': 173,\n",
       " 'fittingly': 174,\n",
       " 'ends': 175,\n",
       " 'drab': 176,\n",
       " 'attempt': 177,\n",
       " 'at': 178,\n",
       " 'comedy': 179,\n",
       " 'hohum': 180,\n",
       " 'an': 181,\n",
       " 'inventorshorst': 182,\n",
       " 'buchholzdeadly': 183,\n",
       " 'biological': 184,\n",
       " 'weapon': 185,\n",
       " 'danger': 186,\n",
       " 'of': 187,\n",
       " 'falling': 188,\n",
       " 'into': 189,\n",
       " 'wrong': 190,\n",
       " 'hands': 191,\n",
       " 'unknowingly': 192,\n",
       " 'sonluke': 193,\n",
       " 'perryhas': 194,\n",
       " 'been': 195,\n",
       " 'working': 196,\n",
       " 'antedote': 197,\n",
       " 'all': 198,\n",
       " 'along': 199,\n",
       " 'enter': 200,\n",
       " 'cia': 201,\n",
       " 'agent': 202,\n",
       " 'olivia': 203,\n",
       " 'dabo': 204,\n",
       " 'catandmouse': 205,\n",
       " 'car': 206,\n",
       " 'chases': 207,\n",
       " 'gunfire': 208,\n",
       " 'begins': 209,\n",
       " 'also': 210,\n",
       " 'cast': 211,\n",
       " 'aretom': 212,\n",
       " 'conti': 213,\n",
       " 'hendrick': 214,\n",
       " 'haese': 215,\n",
       " 'aging': 216,\n",
       " 'roger': 217,\n",
       " 'moore': 218,\n",
       " 'seems': 219,\n",
       " 'haggardly': 220,\n",
       " 'move': 221,\n",
       " 'through': 222,\n",
       " 'mess': 223,\n",
       " 'better': 224,\n",
       " 'efforts': 225,\n",
       " 'perry': 226,\n",
       " 'fans': 227,\n",
       " 'accepting': 228,\n",
       " 'nice': 229,\n",
       " 'look': 230,\n",
       " 'worth': 231,\n",
       " 'rental': 232,\n",
       " 'if': 233,\n",
       " 'you': 234,\n",
       " 'catch': 235,\n",
       " 'cable': 236,\n",
       " 'youll': 237,\n",
       " 'pleasantly': 238,\n",
       " 'surprised': 239,\n",
       " 'cameosimans': 240,\n",
       " 'appearance': 241,\n",
       " 'selfdeprecating': 242,\n",
       " 'its': 243,\n",
       " 'opportunity': 244,\n",
       " 'watch': 245,\n",
       " 'male': 246,\n",
       " 'supporting': 247,\n",
       " 'members': 248,\n",
       " 'from': 249,\n",
       " 'sopranos': 250,\n",
       " 'typecast': 251,\n",
       " 'themselves': 252,\n",
       " 'only': 253,\n",
       " 'children': 254,\n",
       " 'below': 255,\n",
       " 'age': 256,\n",
       " '12': 257,\n",
       " 'should': 258,\n",
       " 'allowed': 259,\n",
       " 'see': 260,\n",
       " 'rest': 261,\n",
       " 'us': 262,\n",
       " 'book': 263,\n",
       " 'mp3': 264,\n",
       " 'player': 265,\n",
       " 'just': 266,\n",
       " 'nap': 267,\n",
       " 'endure': 268,\n",
       " 'experience': 269,\n",
       " 'event': 270,\n",
       " 'can': 271,\n",
       " 'summed': 272,\n",
       " 'blownup': 273,\n",
       " 'tv': 274,\n",
       " 'being': 275,\n",
       " 'distributed': 276,\n",
       " 'theaters': 277,\n",
       " 'want': 278,\n",
       " 'like': 279,\n",
       " 'amused': 280,\n",
       " 'national': 281,\n",
       " 'lampoons': 282,\n",
       " 'class': 283,\n",
       " 'reunion': 284,\n",
       " 'flop': 285,\n",
       " 'when': 286,\n",
       " 'released': 287,\n",
       " 'stay': 288,\n",
       " 'long': 289,\n",
       " 'my': 290,\n",
       " 'big': 291,\n",
       " 'city': 292,\n",
       " 'why': 293,\n",
       " 'because': 294,\n",
       " 'badbr': 295,\n",
       " 'good': 296,\n",
       " 'painfully': 297,\n",
       " 'unfunny': 298,\n",
       " 'entire': 299,\n",
       " 'actors': 300,\n",
       " 'were': 301,\n",
       " 'mostly': 302,\n",
       " 'unknowns': 303,\n",
       " 'then': 304,\n",
       " 'remained': 305,\n",
       " 'after': 306,\n",
       " 'turkeybr': 307,\n",
       " 'idea': 308,\n",
       " 'parody': 309,\n",
       " 'slasher': 310,\n",
       " 'flicks': 311,\n",
       " 'execution': 312,\n",
       " 'sorely': 313,\n",
       " 'lacking': 314,\n",
       " 'aspect': 315,\n",
       " 'your': 316,\n",
       " 'time': 317,\n",
       " 'precious': 318,\n",
       " 'than': 319,\n",
       " 'spending': 320,\n",
       " 'nanosecond': 321,\n",
       " 'watching': 322,\n",
       " 'embarrassing': 323,\n",
       " 'misfire': 324,\n",
       " 'watched': 325,\n",
       " 'word': 326,\n",
       " 'sucky': 327,\n",
       " 'story': 328,\n",
       " 'acting': 329,\n",
       " 'possible': 330,\n",
       " 'worse': 331,\n",
       " 'has': 332,\n",
       " 'two': 333,\n",
       " 'moments': 334,\n",
       " 'thats': 335,\n",
       " 'having': 336,\n",
       " 'those': 337,\n",
       " 'small': 338,\n",
       " 'doesnt': 339,\n",
       " 'anything': 340,\n",
       " 'between': 341,\n",
       " 'before': 342,\n",
       " 'montrocity': 343,\n",
       " 'apart': 344,\n",
       " 'helen': 345,\n",
       " 'bonham': 346,\n",
       " 'carter': 347,\n",
       " 'nothing': 348,\n",
       " 'worthy': 349,\n",
       " 'about': 350,\n",
       " 'movieand': 351,\n",
       " 'surprise': 352,\n",
       " 'ending': 353,\n",
       " 'thought': 354,\n",
       " 'sequel': 355,\n",
       " 'annoying': 356,\n",
       " 'save': 357,\n",
       " 'money': 358,\n",
       " 'wait': 359,\n",
       " 'video': 360,\n",
       " 'ignore': 361,\n",
       " 'scifi': 362,\n",
       " 'adventure': 363,\n",
       " 'means': 364,\n",
       " 'worst': 365,\n",
       " 'agree': 366,\n",
       " 'statement': 367,\n",
       " 'comical': 368,\n",
       " 'bizarre': 369,\n",
       " 'pink': 370,\n",
       " 'tinting': 371,\n",
       " 'unusual': 372,\n",
       " 'special': 373,\n",
       " 'effects': 374,\n",
       " 'favorite': 375,\n",
       " 'late': 376,\n",
       " 'show': 377,\n",
       " 'viewers': 378,\n",
       " 'space': 379,\n",
       " 'explorers': 380,\n",
       " 'planet': 381,\n",
       " 'mars': 382,\n",
       " 'fight': 383,\n",
       " 'off': 384,\n",
       " 'strange': 385,\n",
       " 'giant': 386,\n",
       " 'amoebalike': 387,\n",
       " 'monsters': 388,\n",
       " 'other': 389,\n",
       " 'creatures': 390,\n",
       " 'pretty': 391,\n",
       " 'coolbr': 392,\n",
       " 'includes': 393,\n",
       " 'les': 394,\n",
       " 'tremayne': 395,\n",
       " 'naura': 396,\n",
       " 'hayden': 397,\n",
       " 'gerald': 398,\n",
       " 'mohr': 399,\n",
       " 'jack': 400,\n",
       " 'kruschen': 401,\n",
       " 'get': 402,\n",
       " 'comfy': 403,\n",
       " 'enjoy': 404,\n",
       " 'feel': 405,\n",
       " 'nod': 406,\n",
       " 'moment': 407,\n",
       " 'adding': 408,\n",
       " 'list': 409,\n",
       " 'cult': 410,\n",
       " 'classics': 411,\n",
       " 'miss': 412,\n",
       " 'loose': 413,\n",
       " '100': 414,\n",
       " 'iq': 415,\n",
       " 'points': 416,\n",
       " 'tuning': 417,\n",
       " 'awful': 418,\n",
       " 'refuse': 419,\n",
       " 'tune': 420,\n",
       " 'what': 421,\n",
       " 'ive': 422,\n",
       " 'commercials': 423,\n",
       " 'where': 424,\n",
       " 'did': 425,\n",
       " 'dig': 426,\n",
       " 'guy': 427,\n",
       " 'anyway': 428,\n",
       " 'do': 429,\n",
       " 'intend': 430,\n",
       " 'next': 431,\n",
       " 'season': 432,\n",
       " 'secret': 433,\n",
       " 'out': 434,\n",
       " 'everyone': 435,\n",
       " 'already': 436,\n",
       " 'knows': 437,\n",
       " 'set': 438,\n",
       " 'going': 439,\n",
       " 'people': 440,\n",
       " 'living': 441,\n",
       " 'under': 442,\n",
       " 'rock': 443,\n",
       " 'star': 444,\n",
       " 'stupid': 445,\n",
       " 'women': 446,\n",
       " 'wonder': 447,\n",
       " 'america': 448,\n",
       " 'outsiderslook': 449,\n",
       " 'hated': 450,\n",
       " 'so': 451,\n",
       " 'much': 452,\n",
       " 'remember': 453,\n",
       " 'vividly': 454,\n",
       " 'funny': 455,\n",
       " 'any': 456,\n",
       " 'sex': 457,\n",
       " 'racism': 458,\n",
       " 'humor': 459,\n",
       " 'does': 460,\n",
       " 'deserve': 461,\n",
       " 'costs': 462,\n",
       " 'first': 463,\n",
       " 'half': 464,\n",
       " 'hour': 465,\n",
       " 'rob': 466,\n",
       " 'schneider': 467,\n",
       " 'drinks': 468,\n",
       " 'carton': 469,\n",
       " 'rancid': 470,\n",
       " 'milk': 471,\n",
       " 'could': 472,\n",
       " 'think': 473,\n",
       " 'he': 474,\n",
       " 'deserves': 475,\n",
       " 'making': 476,\n",
       " 'such': 477,\n",
       " 'waste': 478,\n",
       " 'laughed': 479,\n",
       " 'twice': 480,\n",
       " 'line': 481,\n",
       " 'main': 482,\n",
       " 'character': 483,\n",
       " 'says': 484,\n",
       " 'something': 485,\n",
       " 'streets': 486,\n",
       " 'forget': 487,\n",
       " 'probably': 488,\n",
       " 'beginningbr': 489,\n",
       " 'thinnest': 490,\n",
       " 'ever': 491,\n",
       " 'hollywood': 492,\n",
       " 'realize': 493,\n",
       " 'kind': 494,\n",
       " 'degrading': 495,\n",
       " 'sad': 496,\n",
       " 'insult': 497,\n",
       " 'yourself': 498,\n",
       " 'many': 499,\n",
       " 'timesbr': 500,\n",
       " '210': 501,\n",
       " 'dislike': 502,\n",
       " 'childrens': 503,\n",
       " 'tearjerker': 504,\n",
       " 'few': 505,\n",
       " 'redeeming': 506,\n",
       " 'qualities': 507,\n",
       " 'mj': 508,\n",
       " 'fox': 509,\n",
       " 'perfect': 510,\n",
       " 'voice': 511,\n",
       " 'stuart': 512,\n",
       " 'talent': 513,\n",
       " 'wasted': 514,\n",
       " 'hugh': 515,\n",
       " 'laurie': 516,\n",
       " 'amazingly': 517,\n",
       " 'given': 518,\n",
       " 'chance': 519,\n",
       " 'sugarcoated': 520,\n",
       " 'sugar': 521,\n",
       " 'would': 522,\n",
       " 'hardly': 523,\n",
       " 'appeal': 524,\n",
       " 'anyone': 525,\n",
       " 'over': 526,\n",
       " '7': 527,\n",
       " 'years': 528,\n",
       " 'toy': 529,\n",
       " 'inc': 530,\n",
       " 'shrek': 531,\n",
       " 'instead': 532,\n",
       " '310': 533,\n",
       " 'how': 534,\n",
       " 'young': 535,\n",
       " 'girl': 536,\n",
       " 'copes': 537,\n",
       " 'poverty': 538,\n",
       " 'grows': 539,\n",
       " 'her': 540,\n",
       " 'maturity': 541,\n",
       " 'however': 542,\n",
       " 'most': 543,\n",
       " 'subject': 544,\n",
       " 'explored': 545,\n",
       " 'adequately': 546,\n",
       " 'instances': 547,\n",
       " 'sophistication': 548,\n",
       " 'done': 549,\n",
       " 'here': 550,\n",
       " 'fixated': 551,\n",
       " 'breasts': 552,\n",
       " 'soon': 553,\n",
       " 'became': 554,\n",
       " 'boring': 555,\n",
       " 'lost': 556,\n",
       " 'interest': 557,\n",
       " 'wouldve': 558,\n",
       " 'switched': 559,\n",
       " 'latest': 560,\n",
       " 'news': 561,\n",
       " 'starr': 562,\n",
       " 'report': 563,\n",
       " 'found': 564,\n",
       " 'underrated': 565,\n",
       " 'police': 566,\n",
       " 'acadmey': 567,\n",
       " 'mission': 568,\n",
       " 'moscow': 569,\n",
       " 'never': 570,\n",
       " 'maybe': 571,\n",
       " 'spoof': 572,\n",
       " 'made': 573,\n",
       " 'very': 574,\n",
       " 'boringand': 575,\n",
       " 'dumb': 576,\n",
       " 'beyond': 577,\n",
       " 'belief': 578,\n",
       " 'god': 579,\n",
       " 'give': 580,\n",
       " 'hope': 581,\n",
       " 'concordenew': 582,\n",
       " 'horizons': 583,\n",
       " 'wasnt': 584,\n",
       " 'trying': 585,\n",
       " 'serious': 586,\n",
       " 'horror': 587,\n",
       " 'action': 588,\n",
       " 'carnosaur': 589,\n",
       " 'flatout': 590,\n",
       " 'silly': 591,\n",
       " 'start': 592,\n",
       " 'finish': 593,\n",
       " 'c3': 594,\n",
       " 'high': 595,\n",
       " 'water': 596,\n",
       " 'mark': 597,\n",
       " 'genre': 598,\n",
       " 'disappointed': 599,\n",
       " 'waters': 600,\n",
       " 'saw': 601,\n",
       " 'serial': 602,\n",
       " 'mom': 603,\n",
       " 'loved': 604,\n",
       " 'pecker': 605,\n",
       " 'polyester': 606,\n",
       " 'sort': 607,\n",
       " 'thing': 608,\n",
       " 'liked': 609,\n",
       " 'divine': 610,\n",
       " 'shehe': 611,\n",
       " 'had': 612,\n",
       " 'hell': 613,\n",
       " 'lot': 614,\n",
       " 'truly': 615,\n",
       " 'whole': 616,\n",
       " 'wouldnt': 617,\n",
       " 'recommend': 618,\n",
       " 'jameson': 619,\n",
       " 'parker': 620,\n",
       " 'marilyn': 621,\n",
       " 'hassett': 622,\n",
       " 'screens': 623,\n",
       " 'unbelievable': 624,\n",
       " 'couple': 625,\n",
       " 'john': 626,\n",
       " 'travolta': 627,\n",
       " 'lily': 628,\n",
       " 'tomlin': 629,\n",
       " 'larry': 630,\n",
       " 'peerces': 631,\n",
       " 'direction': 632,\n",
       " 'wavers': 633,\n",
       " 'uncontrollably': 634,\n",
       " 'black': 635,\n",
       " 'farce': 636,\n",
       " 'roman': 637,\n",
       " 'tragedy': 638,\n",
       " 'robert': 639,\n",
       " 'klein': 640,\n",
       " 'certainly': 641,\n",
       " 'former': 642,\n",
       " 'selfcentered': 643,\n",
       " 'performance': 644,\n",
       " 'minor': 645,\n",
       " 'underscores': 646,\n",
       " 'total': 647,\n",
       " 'lack': 648,\n",
       " 'balance': 649,\n",
       " 'chemistry': 650,\n",
       " 'normally': 651,\n",
       " 'let': 652,\n",
       " 'myself': 653,\n",
       " 'ascerbic': 654,\n",
       " 'bell': 655,\n",
       " 'jar': 656,\n",
       " 'alltime': 657,\n",
       " 'books': 658,\n",
       " 'makes': 659,\n",
       " 'literally': 660,\n",
       " 'crazy': 661,\n",
       " 'wow': 662,\n",
       " 'snoozer': 663,\n",
       " 'definately': 664,\n",
       " 'bacons': 665,\n",
       " 'films': 666,\n",
       " 'coupled': 667,\n",
       " 'formulatic': 668,\n",
       " 'incredulous': 669,\n",
       " 'yearn': 670,\n",
       " 'viewing': 671,\n",
       " 'television': 672,\n",
       " 'back': 673,\n",
       " 'say': 674,\n",
       " 'basketball': 675,\n",
       " 'scout': 676,\n",
       " 'gets': 677,\n",
       " 'attached': 678,\n",
       " 'person': 679,\n",
       " 'hes': 680,\n",
       " 'recruiting': 681,\n",
       " 'happens': 682,\n",
       " 'belong': 683,\n",
       " 'tribe': 684,\n",
       " 'verge': 685,\n",
       " 'war': 686,\n",
       " 'decided': 687,\n",
       " 'spoiler': 688,\n",
       " 'game': 689,\n",
       " 'grade': 690,\n",
       " 'f': 691,\n",
       " 'shall': 692,\n",
       " 'documentary': 693,\n",
       " 'forbidden': 694,\n",
       " 'germany': 695,\n",
       " 'lie': 696,\n",
       " 'beginning': 697,\n",
       " 'end': 698,\n",
       " 'doubt': 699,\n",
       " 'trash': 700,\n",
       " 'jews': 701,\n",
       " 'supposed': 702,\n",
       " 'killed': 703,\n",
       " 'concentration': 704,\n",
       " 'camps': 705,\n",
       " 'producers': 706,\n",
       " 'offer': 707,\n",
       " 'pay': 708,\n",
       " 'funeral': 709,\n",
       " 'expenses': 710,\n",
       " 'dies': 711,\n",
       " 'fright': 712,\n",
       " 'while': 713,\n",
       " 'offered': 714,\n",
       " 'intensive': 715,\n",
       " 'psychotherapy': 716,\n",
       " 'enjoyed': 717,\n",
       " 'stinker': 718,\n",
       " 'moves': 719,\n",
       " 'house': 720,\n",
       " 'woman': 721,\n",
       " 'looks': 722,\n",
       " 'lived': 723,\n",
       " 'extremely': 724,\n",
       " 'predictable': 725,\n",
       " 'ended': 726,\n",
       " 'caring': 727,\n",
       " 'moviebr': 728,\n",
       " 'got': 729,\n",
       " 'interesting': 730,\n",
       " 'down': 731,\n",
       " 'road': 732,\n",
       " 'convoluted': 733,\n",
       " 'poor': 734,\n",
       " 'illustration': 735,\n",
       " 'ancient': 736,\n",
       " 'magic': 737,\n",
       " 'rituals': 738,\n",
       " 'lead': 739,\n",
       " 'though': 740,\n",
       " 'comparison': 741,\n",
       " 'boomerang': 742,\n",
       " 'meets': 743,\n",
       " 'extremities': 744,\n",
       " 'moral': 745,\n",
       " 'dilemma': 746,\n",
       " 'faced': 747,\n",
       " 'intriguing': 748,\n",
       " 'due': 749,\n",
       " 'writing': 750,\n",
       " 'casting': 751,\n",
       " 'potentially': 752,\n",
       " 'winning': 753,\n",
       " 'premise': 754,\n",
       " 'plundered': 755,\n",
       " '2': 756,\n",
       " '4': 757,\n",
       " 'theater': 758,\n",
       " 'left': 759,\n",
       " '45': 760,\n",
       " 'minutes': 761,\n",
       " 'laughing': 762,\n",
       " 'stuck': 763,\n",
       " 'admit': 764,\n",
       " 'played': 765,\n",
       " 'blackjack': 766,\n",
       " 'cell': 767,\n",
       " 'phone': 768,\n",
       " 'last': 769,\n",
       " '30': 770,\n",
       " 'bearable': 771,\n",
       " 'cant': 772,\n",
       " 'believe': 773,\n",
       " 'life': 774,\n",
       " 'times': 775,\n",
       " 'someone': 776,\n",
       " 'paid': 777,\n",
       " 'again': 778,\n",
       " 'horrible': 779,\n",
       " 'sense': 780,\n",
       " 'couldnt': 781,\n",
       " 'tell': 782,\n",
       " 'terrible': 783,\n",
       " 'rating': 784,\n",
       " 'pure': 785,\n",
       " 'failure': 786,\n",
       " 'am': 787,\n",
       " 'steve': 788,\n",
       " 'martin': 789,\n",
       " 'fan': 790,\n",
       " 'tired': 791,\n",
       " 'swiss': 792,\n",
       " 'cheese': 793,\n",
       " 'academy': 794,\n",
       " 'apply': 795,\n",
       " 'military': 796,\n",
       " 'yuckbr': 797,\n",
       " 'user': 798,\n",
       " 'comments': 799,\n",
       " 'reflected': 800,\n",
       " 'received': 801,\n",
       " 'rightfully': 802,\n",
       " 'deserved': 803,\n",
       " 'misleading': 804,\n",
       " 'marked': 805,\n",
       " '300': 806,\n",
       " 'grocery': 807,\n",
       " 'store': 808,\n",
       " 'now': 809,\n",
       " 'know': 810,\n",
       " 'whybr': 811,\n",
       " '90': 812,\n",
       " 'huge': 813,\n",
       " 'original': 814,\n",
       " 'cartoon': 815,\n",
       " 'series': 816,\n",
       " 'looking': 817,\n",
       " 'forward': 818,\n",
       " 'finally': 819,\n",
       " 'seeing': 820,\n",
       " 'gadget': 821,\n",
       " 'screen': 822,\n",
       " 'wildest': 823,\n",
       " 'dreams': 824,\n",
       " 'expected': 825,\n",
       " 'pace': 826,\n",
       " 'fast': 827,\n",
       " 'wowser': 828,\n",
       " 'wowsers': 829,\n",
       " '5': 830,\n",
       " 'together': 831,\n",
       " 'each': 832,\n",
       " 'write': 833,\n",
       " 'different': 834,\n",
       " 'try': 835,\n",
       " 'western': 836,\n",
       " 'absolutely': 837,\n",
       " 'stinks': 838,\n",
       " 'giving': 839,\n",
       " 'awefully': 840,\n",
       " 'said': 841,\n",
       " 'barely': 842,\n",
       " 'routine': 843,\n",
       " 'mysterythriller': 844,\n",
       " 'concerning': 845,\n",
       " 'killer': 846,\n",
       " 'lurks': 847,\n",
       " 'swamps': 848,\n",
       " 'during': 849,\n",
       " 'early': 850,\n",
       " 'days': 851,\n",
       " 'shown': 852,\n",
       " 'dad': 853,\n",
       " 'whats': 854,\n",
       " 'tonight': 855,\n",
       " 'wed': 856,\n",
       " 'him': 857,\n",
       " 'strangler': 858,\n",
       " 'swamp': 859,\n",
       " 'hed': 860,\n",
       " 'pack': 861,\n",
       " 'we': 862,\n",
       " 'went': 863,\n",
       " 'storyline': 864,\n",
       " 'absurd': 865,\n",
       " 'lamealso': 866,\n",
       " 'sucking': 867,\n",
       " 'performances': 868,\n",
       " 'hard': 869,\n",
       " 'keep': 870,\n",
       " 'eyes': 871,\n",
       " 'open': 872,\n",
       " 'advise': 873,\n",
       " 'caffeinepropelled': 874,\n",
       " 'friend': 875,\n",
       " 'handy': 876,\n",
       " 'wake': 877,\n",
       " 'goreeffectswhy': 878,\n",
       " 'bring': 879,\n",
       " 'alcatraz': 880,\n",
       " 'inin': 881,\n",
       " 'casebecomes': 882,\n",
       " 'increasingly': 883,\n",
       " 'difficult': 884,\n",
       " 'swallow': 885,\n",
       " 'wondered': 886,\n",
       " 'aimed': 887,\n",
       " 'forchock': 888,\n",
       " 'full': 889,\n",
       " 'lame': 890,\n",
       " 'subplots': 891,\n",
       " 'cannibalism': 892,\n",
       " 'armycaptainthis': 893,\n",
       " 'lowgrade': 894,\n",
       " 'aspectbtw': 895,\n",
       " 'banned': 896,\n",
       " 'cq': 897,\n",
       " 'campy': 898,\n",
       " 'fun': 899,\n",
       " 'commits': 900,\n",
       " 'unforgivable': 901,\n",
       " 'sin': 902,\n",
       " 'deadly': 903,\n",
       " 'slow': 904,\n",
       " 'confused': 905,\n",
       " 'artificial': 906,\n",
       " 'impossible': 907,\n",
       " 'care': 908,\n",
       " 'acceptable': 909,\n",
       " 'creative': 910,\n",
       " 'thoughtful': 911,\n",
       " 'sensitive': 912,\n",
       " 'eighth': 913,\n",
       " 'grader': 914,\n",
       " 'carried': 915,\n",
       " 'warning': 916,\n",
       " 'label': 917,\n",
       " 'caution': 918,\n",
       " 'student': 919,\n",
       " 'fit': 920,\n",
       " 'relatives': 921,\n",
       " 'maker': 922,\n",
       " 'whenever': 923,\n",
       " 'columbo': 924,\n",
       " 'deviates': 925,\n",
       " 'familiar': 926,\n",
       " 'colorful': 927,\n",
       " 'crime': 928,\n",
       " 'smokes': 929,\n",
       " 'becomes': 930,\n",
       " 'pest': 931,\n",
       " 'process': 932,\n",
       " 'somehow': 933,\n",
       " 'able': 934,\n",
       " 'match': 935,\n",
       " 'quality': 936,\n",
       " 'traditional': 937,\n",
       " 'episodes': 938,\n",
       " 'episode': 939,\n",
       " 'extreme': 940,\n",
       " 'result': 941,\n",
       " 'major': 942,\n",
       " 'flopbr': 943,\n",
       " 'faces': 944,\n",
       " 'villain': 945,\n",
       " 'till': 946,\n",
       " 'endbr': 947,\n",
       " 'frankly': 948,\n",
       " 'tempted': 949,\n",
       " 'turn': 950,\n",
       " 'twothirds': 951,\n",
       " 'throughbr': 952,\n",
       " 'oh': 953,\n",
       " 'sacrifices': 954,\n",
       " 'selfappointed': 955,\n",
       " 'reviewers': 956,\n",
       " 'piece': 957,\n",
       " 'aint': 958,\n",
       " 'comment': 959,\n",
       " 'simply': 960,\n",
       " 'cry': 961,\n",
       " 'shame': 962,\n",
       " 'these': 963,\n",
       " 'decent': 964,\n",
       " 'matter': 965,\n",
       " 'become': 966,\n",
       " 'belowaverage': 967,\n",
       " 'runofthemill': 968,\n",
       " 'tvmovie': 969,\n",
       " 'week': 970,\n",
       " 'stale': 971,\n",
       " 'nonexistent': 972,\n",
       " 'same': 973,\n",
       " 'le': 974,\n",
       " 'huitime': 975,\n",
       " 'jour': 976,\n",
       " 'cares': 977,\n",
       " 'downsyndrome': 978,\n",
       " 'merely': 979,\n",
       " 'used': 980,\n",
       " 'weepy': 981,\n",
       " 'sentimentality': 982,\n",
       " 'appalling': 983,\n",
       " '110': 984,\n",
       " 'beautiful': 985,\n",
       " 'sets': 986,\n",
       " 'albert': 987,\n",
       " 'finney': 988,\n",
       " 'job': 989,\n",
       " 'ruthless': 990,\n",
       " 'father': 991,\n",
       " 'fails': 992,\n",
       " 'jennifer': 993,\n",
       " 'jason': 994,\n",
       " 'leigh': 995,\n",
       " 'jumpy': 996,\n",
       " 'daughter': 997,\n",
       " 'whatever': 998,\n",
       " 'de': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2566c0-015a-4cdf-b88d-b41376df220d",
   "metadata": {},
   "source": [
    "# Remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91ff2b32-2e36-4848-8be9-c7e880d3ddac",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30dd737e-3810-4866-8054-5db39ecddace",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "32681497-e3e4-44f1-a335-6fce79310f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    words = text.split()\n",
    "    fltered_words = [word for word in words if word not in stop_words]\n",
    "    filtered_text = ' '.join(fltered_words)\n",
    "    return filtered_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba2f9a03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This sample sentence, showing stop words filtration.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_text = \"This is a sample sentence, showing off the stop words filtration.\"\n",
    "remove_stopwords(example_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ae56ad-156e-4a0d-be28-d17914bcc475",
   "metadata": {},
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f77df94d-37e1-4615-a8cf-051296763a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ee0ef18b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('house', 'house', 'housing', 'housed')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize(\"house\"), lemmatizer.lemmatize(\"houses\"), lemmatizer.lemmatize(\"housing\"), lemmatizer.lemmatize(\"housed\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ea7ac992-e0c9-4e47-836a-c7a07ee22aa5",
   "metadata": {},
   "source": [
    "lemmatizer.lemmatize('Horse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82697754-3845-4cbd-8f21-9b8624612c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(word):\n",
    "    # lemmatize word without using lemmatizer \n",
    "    \n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eca4561",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c9f0f1fe-e10c-4bc1-8533-b22f7b194f3b",
   "metadata": {},
   "source": [
    "# Word embedding and sentiment analysis model\n",
    "We want to create a model that can say if a movie review is bad or good.\n",
    "\n",
    "- Preprocess the text\n",
    "- Convert text to seqiuence of integers\n",
    "- Create architecture that includes embeddings\n",
    "- Build and train your models\n",
    "- Evaluate preformance\n",
    "\n",
    "Building models from scratch is not something you usually do, but those who would like to dig deeper into the math behind Simple RNN, LSTM and GRU can do it by creating the cells from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67861b1-61e9-4b7f-a525-f27c11093338",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_data(embedded_text):\n",
    "    # All sentences should be of the same lenght, but if a sentence is shorter than the longest, pad it.\n",
    "    return padded_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae3425e-214e-4f0d-89fa-dd61faacbda3",
   "metadata": {},
   "source": [
    "## RNN with tensorflow modules\n",
    "[Simple RNN cell](https://www.tensorflow.org/api_docs/python/tf/keras/layers/SimpleRNN)\n",
    "\n",
    "[Embedding](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac733a4c-b314-4b2e-ae0d-295e3e2671d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_rnn_model():\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01152d72-7062-4d00-b3f4-7d1ae3b30f3e",
   "metadata": {},
   "source": [
    "## RNN from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba7e11b-131c-49b7-9a3c-0fcd363a632e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNCell(tf.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.Wxh =\n",
    "        self.Whh =\n",
    "        self.bh =\n",
    "\n",
    "    def __call__(self, x, h):\n",
    "        h_next = \n",
    "        return h_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9177958f-cbe0-466c-b39e-78a87ce4f1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN Model Class\n",
    "class MyRNNModel(tf.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim=1, sequence_length=100):\n",
    "        super().__init__()\n",
    "        self.embedding =\n",
    "        self.rnn_cell = RNNCell(embedding_dim, hidden_dim)\n",
    "        self.Why = \n",
    "        self.by = \n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = \n",
    "        h = \n",
    "\n",
    "        # Process the input sequence\n",
    "        for t in range(sequence_length):\n",
    "            x_t = x[:, t, :]\n",
    "            h = self.rnn_cell(x_t, h)\n",
    "\n",
    "        y = \n",
    "        return tf.sigmoid(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5433047d-199f-4aab-858c-f4e25bba4550",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model, inputs, targets):\n",
    "    clip_norm = 1.0\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(inputs)\n",
    "        loss = loss_function(targets, predictions)\n",
    "        \n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(model.trainable_variables)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ececbef-f1f6-4d36-97ee-42ce80b3ff95",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((padded_train_data, y)).batch(batch_size)\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    epoch_loss = 0\n",
    "    epoch_accuracy = 0\n",
    "    total_batches = 0\n",
    "\n",
    "    for batch_inputs, batch_targets in train_dataset:\n",
    "        loss = train_step(model, batch_inputs, batch_targets)\n",
    "        epoch_loss += loss.numpy()\n",
    "\n",
    "        # Calculate accuracy\n",
    "        predictions = model(batch_inputs)\n",
    "        accuracy = calculate_accuracy(batch_targets, predictions)\n",
    "        epoch_accuracy += accuracy.numpy()\n",
    "\n",
    "        total_batches += 1\n",
    "\n",
    "    avg_loss = epoch_loss / total_batches\n",
    "    avg_accuracy = epoch_accuracy / total_batches\n",
    "    print(f'Epoch {epoch+1}/{NUM_EPOCHS}, Loss: {avg_loss:.4f}, Accuracy: {avg_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe11f54b-f41a-4f5a-b1f4-5c5390d9bb6a",
   "metadata": {},
   "source": [
    "## LSTM\n",
    "\n",
    "[LSTM Cell](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTMCell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4902ef9-520c-4975-9aff-f89b623104e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm_model():\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198898ec-771e-4b89-b6dc-12944f0c6d19",
   "metadata": {},
   "source": [
    "## LSTM from scrtch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3a92a1-e930-41c5-8625-4b6c32adf328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM Cell Class\n",
    "class LSTMCell(tf.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # Gates: input, forget, cell, output\n",
    "        self.Wi =\n",
    "        self.Wf =\n",
    "        self.Wc =\n",
    "        self.Wo =\n",
    "        self.bi =\n",
    "        self.bf =\n",
    "        self.bc =\n",
    "        self.bo =\n",
    "\n",
    "    def __call__(self, x, h, c):\n",
    "        combined = tf.concat([x, h], 1)\n",
    "\n",
    "        i = \n",
    "        f = \n",
    "        o = \n",
    "        c_ = \n",
    "\n",
    "        c_new = \n",
    "        h_new =\n",
    "\n",
    "        return h_new, c_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40f3545-5a8f-45d7-813e-075ae61dd09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM Model Class\n",
    "class MyLSTMModel(tf.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.embedding =\n",
    "        self.lstm_cell = LSTMCell(embedding_dim, hidden_dim)\n",
    "        self.Why =\n",
    "        self.by =\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x =\n",
    "        h =\n",
    "        c =\n",
    "\n",
    "        for t in range(sequence_length):\n",
    "            x_t = x[:, t, :]\n",
    "            h, c = self.lstm_cell(x_t, h, c)\n",
    "\n",
    "        y =\n",
    "        return tf.sigmoid(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d248f5-0b90-42f7-99ce-351167d00b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model, inputs, targets):\n",
    "    clip_norm = 1.0\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(inputs)\n",
    "        loss = loss_function(targets, predictions)\n",
    "        \n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    clipped_gradients = [tf.clip_by_norm(g, clip_norm) for g in gradients]\n",
    "    optimizer.apply_gradients(model.trainable_variables)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9d6d6e-48d0-4b40-b402-f81b13b62c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((padded_train_data, y)).batch(batch_size)\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    epoch_loss = 0\n",
    "    epoch_accuracy = 0\n",
    "    total_batches = 0\n",
    "\n",
    "    for batch_inputs, batch_targets in train_dataset:\n",
    "        loss = train_step(model, batch_inputs, batch_targets)\n",
    "        epoch_loss += loss.numpy()\n",
    "\n",
    "        # Calculate accuracy\n",
    "        predictions = model(batch_inputs)\n",
    "        accuracy = calculate_accuracy(batch_targets, predictions)\n",
    "        epoch_accuracy += accuracy.numpy()\n",
    "\n",
    "        total_batches += 1\n",
    "\n",
    "    avg_loss = epoch_loss / total_batches\n",
    "    avg_accuracy = epoch_accuracy / total_batches\n",
    "    print(f'Epoch {epoch+1}/{NUM_EPOCHS}, Loss: {avg_loss:.4f}, Accuracy: {avg_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a3810e-0ae8-4058-b749-799b254f3e81",
   "metadata": {},
   "source": [
    "## GRU\n",
    "[GRU Cell](https://www.tensorflow.org/api_docs/python/tf/keras/layers/GRUCell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da681b6-0ae3-4459-87f4-c8184009ea8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_gru_model():\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a06ce2-4f0e-4fb8-b42a-d307982da693",
   "metadata": {},
   "source": [
    "## GRU from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aae6a39-8185-4260-9d6b-e1bb45d2b93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRU Cell Class\n",
    "class GRUCell(tf.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # Update gate parameters\n",
    "        self.Wz =\n",
    "        self.bz =\n",
    "\n",
    "        # Reset gate parameters\n",
    "        self.Wr =\n",
    "        self.br =\n",
    "\n",
    "        # Candidate hidden state parameters\n",
    "        self.Wh =\n",
    "        self.bh =\n",
    "        \n",
    "    def __call__(self, x, h):\n",
    "        combined = tf.concat([x, h], 1)\n",
    "\n",
    "        # Update gate\n",
    "        z =\n",
    "\n",
    "        # Reset gate\n",
    "        r =\n",
    "\n",
    "        # Candidate hidden state\n",
    "        combined_reset =\n",
    "        h_candidate =\n",
    "\n",
    "        # New hidden state\n",
    "        h_new =\n",
    "\n",
    "        return h_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7b6bb3-7353-4aa4-ab88-bf008934661d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRU Model Class\n",
    "class MyGRUModel(tf.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.embedding =\n",
    "        self.gru_cell =\n",
    "        self.Why =\n",
    "        self.by =\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x =\n",
    "        h =\n",
    "\n",
    "        for t in range(sequence_length):\n",
    "            x_t = x[:, t, :]\n",
    "            h = self.gru_cell(x_t, h)\n",
    "\n",
    "        y =\n",
    "        return tf.sigmoid(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5653c0e5-4d7c-443a-9bce-566c92fcac26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model, inputs, targets):\n",
    "    clip_norm = 1.0\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(inputs)\n",
    "        loss = loss_function(targets, predictions)\n",
    "        \n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(model.trainable_variables)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b3eae4-cda1-4574-bc05-43869b94fc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((padded_train_data, y)).batch(batch_size)\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    epoch_loss = 0\n",
    "    epoch_accuracy = 0\n",
    "    total_batches = 0\n",
    "\n",
    "    for batch_inputs, batch_targets in train_dataset:\n",
    "        loss = train_step(model, batch_inputs, batch_targets)\n",
    "        epoch_loss += loss.numpy()\n",
    "\n",
    "        # Calculate accuracy\n",
    "        predictions = model(batch_inputs)\n",
    "        accuracy = calculate_accuracy(batch_targets, predictions)\n",
    "        epoch_accuracy += accuracy.numpy()\n",
    "\n",
    "        total_batches += 1\n",
    "\n",
    "    avg_loss = epoch_loss / total_batches\n",
    "    avg_accuracy = epoch_accuracy / total_batches\n",
    "    print(f'Epoch {epoch+1}/{NUM_EPOCHS}, Loss: {avg_loss:.4f}, Accuracy: {avg_accuracy:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deep_learning-ZjjgEFKy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
